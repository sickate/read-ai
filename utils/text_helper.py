import re
from typing import Dict, Any, List
import os
import sys
import time
import socket

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥appæ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.llm.providers import get_provider_config, AliyunModel, GeminiModel


def analyze_text(text: str) -> Dict[str, Any]:
    """
    åˆ†ææ–‡æœ¬ï¼Œç»Ÿè®¡æ±‰å­—å­—æ•°ã€æ ‡ç‚¹ç¬¦å·æ•°å’Œè‹±æ–‡å•è¯æ•°
    
    Args:
        text (str): è¦åˆ†æçš„æ–‡æœ¬
        
    Returns:
        Dict[str, Any]: åŒ…å«ç»Ÿè®¡ç»“æœçš„å­—å…¸
            - chinese_chars: æ±‰å­—å­—æ•°
            - punctuation: æ ‡ç‚¹ç¬¦å·æ•°ï¼ˆåŒ…æ‹¬å…¨è§’å’ŒåŠè§’ï¼‰
            - english_words: è‹±æ–‡å•è¯æ•°
            - total_chars: æ€»å­—ç¬¦æ•°
    """
    if not text:
        return {
            "chinese_chars": 0,
            "punctuation": 0,
            "english_words": 0,
            "total_chars": 0
        }
    
    # ç»Ÿè®¡æ±‰å­— (ä½¿ç”¨å¸¸ç”¨æ±‰å­—çš„UnicodeèŒƒå›´)
    # \u4e00-\u9fff: CJKç»Ÿä¸€æ±‰å­—åŸºæœ¬åŒºåŸŸ
    chinese_pattern = r'[\u4e00-\u9fff]'
    chinese_chars = len(re.findall(chinese_pattern, text))
    
    # ç»Ÿè®¡æ ‡ç‚¹ç¬¦å· (åŒ…æ‹¬ä¸­æ–‡å’Œè‹±æ–‡æ ‡ç‚¹)
    # ä¸­æ–‡æ ‡ç‚¹ç¬¦å·å’Œå…¨è§’ç¬¦å·
    chinese_punctuation_pattern = r'[\u3000-\u303f\uff00-\uffef]'
    # è‹±æ–‡æ ‡ç‚¹ç¬¦å·
    english_punctuation_pattern = r'[!\"#$%&\'()*+,\-./:;<=>?@\[\\\]^_`{|}~]'
    
    # åˆ†åˆ«ç»Ÿè®¡ä¸­è‹±æ–‡æ ‡ç‚¹
    chinese_punctuation_count = len(re.findall(chinese_punctuation_pattern, text))
    english_punctuation_count = len(re.findall(english_punctuation_pattern, text))
    punctuation = chinese_punctuation_count + english_punctuation_count
    
    # ç»Ÿè®¡è‹±æ–‡å•è¯æ•°
    # åŒ¹é…è‹±æ–‡å•è¯ (è¿ç»­çš„å­—æ¯ï¼Œå¯èƒ½åŒ…å«æ’‡å·)
    english_word_pattern = r"[a-zA-Z]+(?:'[a-zA-Z]+)?"
    english_words = len(re.findall(english_word_pattern, text))
    
    # æ€»å­—ç¬¦æ•°
    total_chars = len(text)
    
    return {
        "chinese_chars": chinese_chars,
        "punctuation": punctuation,
        "english_words": english_words,
        "total_chars": total_chars
    }


def format_analysis_result(result: Dict[str, Any]) -> str:
    """
    æ ¼å¼åŒ–åˆ†æç»“æœä¸ºå¯è¯»å­—ç¬¦ä¸²
    
    Args:
        result (Dict[str, Any]): analyze_textå‡½æ•°çš„è¿”å›ç»“æœ
        
    Returns:
        str: æ ¼å¼åŒ–çš„ç»“æœå­—ç¬¦ä¸²
    """
    return f"""
ğŸ“Š æ–‡æœ¬åˆ†æç»“æœ:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ æ±‰å­—å­—æ•°:     {result['chinese_chars']:,}
ğŸ”¤ è‹±æ–‡å•è¯æ•°:   {result['english_words']:,}
ğŸ”£ æ ‡ç‚¹ç¬¦å·æ•°:   {result['punctuation']:,}
ğŸ“„ æ€»å­—ç¬¦æ•°:     {result['total_chars']:,}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""


def get_smart_llm_provider(language: str = "zh"):
    """
    æ ¹æ®ä¸»æœºåå’Œè¯­è¨€æ™ºèƒ½é€‰æ‹©LLMæä¾›è€…å’Œæ¨¡å‹

    Args:
        language (str): è¯­è¨€ä»£ç  "zh", "en", "es"

    Returns:
        tuple: (provider, model)
    """
    hostname = socket.gethostname().lower()

    # å¦‚æœæ˜¯maruæœåŠ¡å™¨ï¼Œä½¿ç”¨é˜¿é‡Œäº‘æ¨¡å‹ï¼ˆå› ä¸ºæ²¡æœ‰VPNæ— æ³•è®¿é—®Geminiï¼‰
    if hostname == "maru":
        provider = get_provider_config('aliyun')
        model = AliyunModel.DEEPSEEK_R1.value
    else:
        # å…¶ä»–ç¯å¢ƒä½¿ç”¨Geminiï¼ˆè½»é‡ä¸”å¤šè¯­è¨€æ”¯æŒå¥½ï¼‰
        try:
            provider = get_provider_config('google')
            model = GeminiModel.GEMINI_2_5_FLASH_LITE.value
        except:
            # å¦‚æœGeminié…ç½®æœ‰é—®é¢˜ï¼Œå›é€€åˆ°é˜¿é‡Œäº‘
            provider = get_provider_config('aliyun')
            model = AliyunModel.DEEPSEEK_R1.value

    return provider, model


def analyze_text_multilingual(text: str, language: str = "zh") -> Dict[str, Any]:
    """
    å¤šè¯­è¨€æ–‡æœ¬åˆ†æï¼Œç»Ÿè®¡ä¸åŒè¯­è¨€çš„å•ä½

    Args:
        text (str): è¦åˆ†æçš„æ–‡æœ¬
        language (str): è¯­è¨€ä»£ç  "zh", "en", "es"

    Returns:
        Dict[str, Any]: åŒ…å«ç»Ÿè®¡ç»“æœçš„å­—å…¸
            - main_count: ä¸»è¦è®¡æ•°å•ä½ï¼ˆä¸­æ–‡å­—æ•°æˆ–è‹±æ–‡/è¥¿è¯­å•è¯æ•°ï¼‰
            - punctuation: æ ‡ç‚¹ç¬¦å·æ•°
            - total_chars: æ€»å­—ç¬¦æ•°
            - language: è¯­è¨€æ ‡è¯†
    """
    if not text:
        return {
            "main_count": 0,
            "punctuation": 0,
            "total_chars": 0,
            "language": language
        }

    # ç»Ÿè®¡æ ‡ç‚¹ç¬¦å·ï¼ˆé€šç”¨ï¼‰
    chinese_punctuation_pattern = r'[\u3000-\u303f\uff00-\uffef]'
    english_punctuation_pattern = r'[!\"#$%&\'()*+,\-./:;<=>?@\[\\\]^_`{|}~]'
    chinese_punctuation_count = len(re.findall(chinese_punctuation_pattern, text))
    english_punctuation_count = len(re.findall(english_punctuation_pattern, text))
    punctuation = chinese_punctuation_count + english_punctuation_count

    # æ€»å­—ç¬¦æ•°
    total_chars = len(text)

    if language == "zh":
        # ä¸­æ–‡ï¼šç»Ÿè®¡æ±‰å­—æ•°
        chinese_pattern = r'[\u4e00-\u9fff]'
        main_count = len(re.findall(chinese_pattern, text))
    else:
        # è‹±è¯­/è¥¿è¯­ï¼šç»Ÿè®¡å•è¯æ•°
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼ŒæŒ‰ç©ºæ ¼åˆ†å‰²å•è¯
        words = re.findall(r'\b\w+\b', text.lower())
        main_count = len(words)

    return {
        "main_count": main_count,
        "punctuation": punctuation,
        "total_chars": total_chars,
        "language": language
    }


def ai_correct_essay(text: str, word_count: str = "ä¸é™å­—æ•°", grade: str = "ä¸‰å¹´çº§", language: str = "zh") -> Dict[str, Any]:
    """
    ä½¿ç”¨AIæ‰¹æ”¹ä½œæ–‡ï¼Œæ£€æŸ¥è¯­æ³•ã€é”™åˆ«å­—ã€æ ‡ç‚¹ç¬¦å·ç­‰é—®é¢˜

    Args:
        text (str): è¦æ‰¹æ”¹çš„ä½œæ–‡å†…å®¹
        word_count (str): ä½œæ–‡å­—æ•°è¦æ±‚
        grade (str): å¹´çº§
        language (str): è¯­è¨€ä»£ç  "zh", "en", "es"

    Returns:
        Dict[str, Any]: åŒ…å«æ‰¹æ”¹ç»“æœçš„å­—å…¸
            - success: æ˜¯å¦æˆåŠŸ
            - corrections: ä¿®æ”¹å»ºè®®åˆ—è¡¨
            - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    """
    if not text or not text.strip():
        return {
            "success": False,
            "error": "ä½œæ–‡å†…å®¹ä¸èƒ½ä¸ºç©º"
        }

    try:
        # æ™ºèƒ½é€‰æ‹©LLMæä¾›è€…
        provider, model = get_smart_llm_provider(language)
        client = provider.get_llm()

        # æ ¹æ®è¯­è¨€é€‰æ‹©æ‰¹æ”¹æç¤ºè¯
        prompt = get_correction_prompt(text, language, word_count, grade)

        # è°ƒç”¨AIæ¨¡å‹
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": get_system_prompt(language)},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=2000
        )

        ai_response = response.choices[0].message.content

        # è§£æAIè¿”å›çš„æ‰¹æ”¹ç»“æœ
        corrections = parse_correction_response(ai_response)

        return {
            "success": True,
            "corrections": corrections,
            "raw_response": ai_response
        }

    except Exception as e:
        return {
            "success": False,
            "error": f"AIæ‰¹æ”¹å¤±è´¥ï¼š{str(e)}"
        }


def get_system_prompt(language: str) -> str:
    """
    æ ¹æ®è¯­è¨€è¿”å›ç³»ç»Ÿæç¤ºè¯

    Args:
        language (str): è¯­è¨€ä»£ç  "zh", "en", "es"

    Returns:
        str: ç³»ç»Ÿæç¤ºè¯
    """
    if language == "en":
        return "You are a professional English teacher who helps Chinese students improve their English writing skills."
    elif language == "es":
        return "You are a professional Spanish teacher who helps Chinese students learn Spanish writing."
    else:
        return "ä½ æ˜¯ä¸€åä¸“ä¸šçš„è¯­æ–‡è€å¸ˆï¼Œè´Ÿè´£æ‰¹æ”¹å­¦ç”Ÿä½œæ–‡ã€‚"


def get_correction_prompt(text: str, language: str, word_count: str, grade: str) -> str:
    """
    æ ¹æ®è¯­è¨€ç”Ÿæˆæ‰¹æ”¹æç¤ºè¯

    Args:
        text (str): ä½œæ–‡å†…å®¹
        language (str): è¯­è¨€ä»£ç  "zh", "en", "es"
        word_count (str): å­—æ•°è¦æ±‚
        grade (str): å¹´çº§

    Returns:
        str: æ‰¹æ”¹æç¤ºè¯
    """
    if language == "en":
        return get_english_correction_prompt(text, word_count, grade)
    elif language == "es":
        return get_spanish_correction_prompt(text, word_count, grade)
    else:
        return get_chinese_correction_prompt(text, word_count, grade)


def get_chinese_correction_prompt(text: str, word_count: str, grade: str) -> str:
    """ç”Ÿæˆä¸­æ–‡ä½œæ–‡æ‰¹æ”¹æç¤ºè¯"""
    grade_requirements = {
        "ä¸€å¹´çº§": "ç®€å•è¯­å¥ï¼ŒåŸºæœ¬è¡¨è¾¾æ¸…æ¥š",
        "äºŒå¹´çº§": "å¥å­é€šé¡ºï¼Œæœ‰åŸºæœ¬çš„æ®µè½æ¦‚å¿µ",
        "ä¸‰å¹´çº§": "è¯­å¥é€šé¡ºï¼Œæœ‰å¼€å¤´ã€ä¸­é—´ã€ç»“å°¾",
        "å››å¹´çº§": "ç»“æ„æ¸…æ¥šï¼Œè¯­è¨€è¾ƒä¸ºæµç•…",
        "äº”å¹´çº§": "å†…å®¹å……å®ï¼Œè¯­è¨€ç”ŸåŠ¨",
        "å…­å¹´çº§": "æ–‡ç« ç»“æ„å®Œæ•´ï¼Œè¯­è¨€å‡†ç¡®ç”ŸåŠ¨",
        "åˆä¸€": "è§‚ç‚¹æ˜ç¡®ï¼Œè®ºè¯æœ‰æ¡ç†",
        "åˆäºŒ": "è¯­è¨€è¡¨è¾¾å‡†ç¡®ï¼Œæœ‰ä¸€å®šæ–‡é‡‡",
        "åˆä¸‰": "æ€æƒ³æ·±åˆ»ï¼Œè¯­è¨€ä¼˜ç¾ï¼Œç»“æ„ä¸¥è°¨"
    }

    grade_req = grade_requirements.get(grade, "è¯­å¥é€šé¡ºï¼Œå†…å®¹å®Œæ•´")
    word_count_req = f"å­—æ•°è¦æ±‚ï¼š{word_count}" if word_count != "ä¸é™å­—æ•°" else "å­—æ•°æ— ç‰¹æ®Šè¦æ±‚"

    return f"""è¯·ä½ ä½œä¸ºä¸€åä¸“ä¸šçš„è¯­æ–‡è€å¸ˆï¼Œä»”ç»†æ‰¹æ”¹ä»¥ä¸‹{grade}å­¦ç”Ÿçš„ä½œæ–‡ã€‚

ã€ä½œæ–‡è¦æ±‚ã€‘
- å¹´çº§ï¼š{grade}
- {word_count_req}
- è¯„åˆ¤æ ‡å‡†ï¼š{grade_req}

ã€æ‰¹æ”¹è¦æ±‚ã€‘
1. ä¸è¦ä¿®æ”¹åŸæ–‡ï¼Œåªæ ‡å‡ºéœ€è¦ä¿®æ”¹çš„åœ°æ–¹
2. ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œæ¯ä¸ª ## æ ‡é¢˜ä¸‹å¿…é¡»æœ‰ - å¼€å¤´çš„åˆ—è¡¨é¡¹ï¼š

ã€è¾“å‡ºæ ¼å¼æ ·ä¾‹ã€‘
## æ€»ä½“è¯„ä»·
- æ€»ä½“å°è±¡ï¼šæ–‡ç« ä¸»é¢˜æ˜ç¡®ï¼Œç»“æ„åŸºæœ¬æ¸…æ™°
- ä¼˜ç‚¹ï¼šå¼€å¤´ç‚¹é¢˜ï¼Œå†…å®¹è¾ƒå……å®
- ä¸»è¦é—®é¢˜ï¼šå­˜åœ¨è¯­æ³•é”™è¯¯å’Œç”¨è¯ä¸å½“
- å»ºè®®å¾—åˆ†ï¼š75åˆ†

## ç—…å¥ä¿®æ”¹
- ç¬¬1æ®µç¬¬2å¥ï¼š"æˆ‘ä»¬å­¦æ ¡å‘ç”Ÿäº†å¾ˆå¤šçš„å˜åŒ–" â†’ é—®é¢˜ï¼šè¯­åºä¸å½“ â†’ å»ºè®®ï¼šæ”¹ä¸º"æˆ‘ä»¬å­¦æ ¡å‘ç”Ÿäº†å¾ˆå¤šå˜åŒ–"

## é”™åˆ«å­—ä¿®æ”¹
- ç¬¬2æ®µï¼š"å³ä½¿"å†™æˆäº†"æ—¢ä½¿" â†’ åº”ä¸ºï¼š"å³ä½¿" â†’ ä½ç½®ï¼šç¬¬2æ®µç¬¬3è¡Œ

## æ ‡ç‚¹ç¬¦å·ä¿®æ”¹
- ç¬¬1æ®µï¼šå¥å·ä½¿ç”¨é”™è¯¯ â†’ å»ºè®®ï¼šç–‘é—®å¥åº”ç”¨é—®å·

## è¯­è¨€è¡¨è¾¾æ”¹è¿›å»ºè®®
- å»ºè®®ä½¿ç”¨æ›´ä¸°å¯Œçš„è¯æ±‡æ¥è¡¨è¾¾æƒ…æ„Ÿ
- å¯ä»¥é€‚å½“è¿ç”¨ä¿®è¾æ‰‹æ³•å¢å¼ºè¡¨è¾¾æ•ˆæœ

## å†…å®¹ç»“æ„æ”¹è¿›å»ºè®®
- æ®µè½ä¹‹é—´ç¼ºä¹è¿‡æ¸¡ï¼Œå»ºè®®æ·»åŠ æ‰¿æ¥è¯è¯­
- ç»“å°¾å¯ä»¥æ›´å¥½åœ°å‘¼åº”å¼€å¤´

ã€ä½œæ–‡å†…å®¹ã€‘
{text}

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼æ‰¹æ”¹ï¼š"""


def get_english_correction_prompt(text: str, word_count: str, grade: str) -> str:
    """ç”Ÿæˆè‹±è¯­ä½œæ–‡æ‰¹æ”¹æç¤ºè¯"""
    grade_levels = {
        "ä¸€å¹´çº§": "åˆå­¦è€…æ°´å¹³ - åŸºç¡€å¥å‹å’Œç®€å•è¯æ±‡",
        "äºŒå¹´çº§": "å…¥é—¨çº§ - ä¸€èˆ¬ç°åœ¨æ—¶å’ŒåŸºç¡€å¥å­ç»“æ„",
        "ä¸‰å¹´çº§": "åˆçº§ - è¿‡å»æ—¶ã€å°†æ¥æ—¶å’ŒåŸºç¡€æ®µè½",
        "å››å¹´çº§": "åˆä¸­çº§ - å¤æ‚å¥å¼å’Œè¯æ±‡æ‰©å±•",
        "äº”å¹´çº§": "ä¸­çº§ - æ–‡ç« ç»“æ„ï¼ˆå¼€å¤´ã€æ­£æ–‡ã€ç»“å°¾ï¼‰",
        "å…­å¹´çº§": "ä¸­é«˜çº§ - é«˜çº§è¯æ±‡å’Œå¤šæ ·å¥å‹",
        "åˆä¸€": "ä¸­çº§è¿›é˜¶ - ä¸­ç­‰è¯­æ³•å’Œæ–‡ç« ç»„ç»‡",
        "åˆäºŒ": "é«˜çº§ - é«˜çº§è¯­æ³•å’Œå†™ä½œæŠ€å·§",
        "åˆä¸‰": "é«˜çº§è¿›é˜¶ - å¤æ‚å†™ä½œå’Œç²¾å‡†è¡¨è¾¾"
    }

    level_desc = grade_levels.get(grade, "ä¸­çº§æ°´å¹³")
    word_req = f"å•è¯æ•°è¦æ±‚ï¼š{word_count}ä¸ªå•è¯" if word_count != "ä¸é™å­—æ•°" else "æ— ç‰¹æ®Šå•è¯æ•°è¦æ±‚"

    return f"""ä½œä¸ºä¸€åä¸“ä¸šçš„è‹±è¯­è€å¸ˆï¼Œè¯·ä»”ç»†æ‰¹æ”¹ä»¥ä¸‹ä¸­å›½å­¦ç”Ÿçš„è‹±è¯­ä½œæ–‡ã€‚è¯·ç”¨ä¸­æ–‡è¿›è¡Œæ‰¹æ”¹å’Œå»ºè®®ï¼Œå¸®åŠ©å­¦ç”Ÿæ›´å¥½åœ°ç†è§£ã€‚

ã€ä½œæ–‡è¦æ±‚ã€‘
- æ°´å¹³ï¼š{level_desc}
- {word_req}

ã€æ‰¹æ”¹è¦æ±‚ã€‘
1. ä¸è¦é‡å†™ä½œæ–‡ï¼ŒåªæŒ‡å‡ºéœ€è¦æ”¹è¿›çš„åœ°æ–¹
2. ç”¨ä¸­æ–‡è§£é‡Šè‹±è¯­é”™è¯¯ï¼Œä¾¿äºå­¦ç”Ÿç†è§£
3. é‡ç‚¹å…³æ³¨ä¸­å›½å­¦ç”Ÿå¸¸è§çš„è‹±è¯­é”™è¯¯ï¼ˆå¦‚å† è¯ã€æ—¶æ€ã€ä¸­å¼è‹±è¯­ç­‰ï¼‰
4. ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œæ¯ä¸ª ## æ ‡é¢˜ä¸‹å¿…é¡»æœ‰ - å¼€å¤´çš„åˆ—è¡¨é¡¹ï¼š

ã€è¾“å‡ºæ ¼å¼æ ·ä¾‹ã€‘
## æ€»ä½“è¯„ä»·
- æ€»ä½“å°è±¡ï¼šæ–‡ç« ä¸»é¢˜æ˜ç¡®ï¼Œä½†å­˜åœ¨è¯­æ³•é”™è¯¯
- ä¼˜ç‚¹ï¼šè¯æ±‡é‡ä¸é”™ï¼Œé€»è¾‘è¾ƒæ¸…æ™°
- ä¸»è¦é—®é¢˜ï¼šæ—¶æ€ä½¿ç”¨ä¸å‡†ç¡®ï¼Œä¸­å¼è‹±è¯­è¡¨è¾¾è¾ƒå¤š
- å»ºè®®å¾—åˆ†ï¼šBçº§

## è¯­æ³•é”™è¯¯ä¿®æ­£
- ç¬¬1æ®µç¬¬1å¥ï¼š"I am study English" â†’ é”™è¯¯ç±»å‹ï¼šæ—¶æ€é”™è¯¯ â†’ å»ºè®®ï¼šåº”ä¸º"I am studying English"æˆ–"I study English"

## è¯æ±‡ä½¿ç”¨æ”¹è¿›
- ç¬¬2æ®µï¼š"very good" â†’ å»ºè®®ç”¨è¯ï¼š"excellent" â†’ è¯´æ˜ï¼šé¿å…é‡å¤ä½¿ç”¨ç®€å•è¯æ±‡

## å¥å­ç»“æ„é—®é¢˜
- ç¬¬1æ®µï¼šå¥å­è¿‡é•¿éš¾ç†è§£ â†’ å»ºè®®ï¼šæ‹†åˆ†ä¸ºä¸¤ä¸ªç®€å•å¥

## ä¸­å¼è‹±è¯­ä¿®æ­£
- "I very like it" â†’ åœ°é“è¡¨è¾¾ï¼š"I like it very much" â†’ è§£é‡Šï¼šå‰¯è¯ä½ç½®è¦æ­£ç¡®

## è¯­è¨€è¡¨è¾¾æ”¹è¿›å»ºè®®
- å°è¯•ä½¿ç”¨æ›´å¤šè¿æ¥è¯æ¥å¢å¼ºé€»è¾‘æ€§
- é¿å…é‡å¤ä½¿ç”¨ç›¸åŒçš„å¥å‹ç»“æ„

## å†…å®¹ç»“æ„æ”¹è¿›å»ºè®®
- å¼€å¤´æ®µå¯ä»¥æ›´æ˜ç¡®åœ°ç‚¹å‡ºä¸»é¢˜
- ç»“å°¾æ®µéœ€è¦æ›´å¥½åœ°æ€»ç»“å…¨æ–‡

ã€å­¦ç”Ÿä½œæ–‡ã€‘
{text}

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼æ‰¹æ”¹ï¼š"""


def get_spanish_correction_prompt(text: str, word_count: str, grade: str) -> str:
    """ç”Ÿæˆè¥¿ç­ç‰™è¯­ä½œæ–‡æ‰¹æ”¹æç¤ºè¯"""
    grade_levels = {
        "ä¸€å¹´çº§": "åˆå­¦è€… - åŸºæœ¬è¯æ±‡å’Œç°åœ¨æ—¶",
        "äºŒå¹´çº§": "å…¥é—¨çº§ - ç®€å•å¥å‹å’ŒåŸºç¡€è¯­æ³•",
        "ä¸‰å¹´çº§": "åˆçº§ - è¿‡å»æ—¶å’Œå°†æ¥æ—¶çš„ä½¿ç”¨",
        "å››å¹´çº§": "åˆä¸­çº§ - å¤æ‚å¥å‹å’Œè¯æ±‡æ‰©å±•",
        "äº”å¹´çº§": "ä¸­çº§ - æ–‡ç« ç»“æ„å’Œæ®µè½ç»„ç»‡",
        "å…­å¹´çº§": "ä¸­é«˜çº§ - é«˜çº§è¯­æ³•å’Œè¡¨è¾¾æŠ€å·§",
        "åˆä¸€": "é«˜çº§åˆå­¦ - è™šæ‹Ÿå¼å’Œå¤æ‚æ—¶æ€",
        "åˆäºŒ": "ä¸­çº§è¿›é˜¶ - é«˜çº§å†™ä½œæŠ€å·§",
        "åˆä¸‰": "é«˜çº§ - å¤æ‚å†™ä½œå’Œæ–‡å­¦è¡¨è¾¾"
    }

    level_desc = grade_levels.get(grade, "ä¸­çº§æ°´å¹³")
    word_req = f"å­—æ•°è¦æ±‚ï¼š{word_count}ä¸ªå•è¯" if word_count != "ä¸é™å­—æ•°" else "æ— ç‰¹æ®Šå­—æ•°è¦æ±‚"

    return f"""ä½œä¸ºä¸€åä¸“ä¸šçš„è¥¿ç­ç‰™è¯­è€å¸ˆï¼Œè¯·ä»”ç»†æ‰¹æ”¹ä»¥ä¸‹ä¸­å›½å­¦ç”Ÿçš„è¥¿ç­ç‰™è¯­ä½œæ–‡ã€‚è¯·ç”¨ä¸­æ–‡è¿›è¡Œæ‰¹æ”¹å’Œå»ºè®®ï¼Œå¸®åŠ©å­¦ç”Ÿæ›´å¥½åœ°ç†è§£ã€‚

ã€ä½œæ–‡è¦æ±‚ã€‘
- æ°´å¹³ï¼š{level_desc}
- {word_req}

ã€æ‰¹æ”¹è¦æ±‚ã€‘
1. ä¸è¦é‡å†™ä½œæ–‡ï¼ŒåªæŒ‡å‡ºéœ€è¦æ”¹è¿›çš„åœ°æ–¹
2. ç”¨ä¸­æ–‡è§£é‡Šè¯­æ³•é”™è¯¯ï¼Œä¾¿äºå­¦ç”Ÿç†è§£
3. é‡ç‚¹å…³æ³¨ä¸­å›½å­¦ç”Ÿå¸¸è§çš„è¥¿ç­ç‰™è¯­é”™è¯¯ï¼ˆå¦‚æ€§åˆ«ä¸€è‡´ã€åŠ¨è¯å˜ä½ç­‰ï¼‰
4. ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œæ¯ä¸ª ## æ ‡é¢˜ä¸‹å¿…é¡»æœ‰ - å¼€å¤´çš„åˆ—è¡¨é¡¹ï¼š

ã€è¾“å‡ºæ ¼å¼æ ·ä¾‹ã€‘
## æ€»ä½“è¯„ä»·
- æ€»ä½“å°è±¡ï¼šæ–‡ç« å†…å®¹åŸºæœ¬å®Œæ•´ï¼Œä½†è¯­æ³•é”™è¯¯è¾ƒå¤š
- ä¼˜ç‚¹ï¼šè¯æ±‡ä½¿ç”¨è¾ƒä¸°å¯Œï¼Œä¸»é¢˜æ˜ç¡®
- ä¸»è¦é—®é¢˜ï¼šåŠ¨è¯å˜ä½å’Œæ€§åˆ«ä¸€è‡´æ€§é”™è¯¯é¢‘ç¹
- å»ºè®®å¾—åˆ†ï¼šCçº§

## è¯­æ³•é”™è¯¯ä¿®æ­£
- ç¬¬1æ®µç¬¬2å¥ï¼š"Yo es estudiante" â†’ é”™è¯¯ç±»å‹ï¼šåŠ¨è¯å˜ä½é”™è¯¯ â†’ å»ºè®®ï¼šåº”ä¸º"Yo soy estudiante"ï¼ˆç¬¬ä¸€äººç§°å•æ•°ç”¨soyï¼‰

## è¯æ±‡ä½¿ç”¨æ”¹è¿›
- ç¬¬2æ®µï¼š"casa pequeÃ±o" â†’ å»ºè®®ç”¨è¯ï¼š"casa pequeÃ±a" â†’ è¯´æ˜ï¼šå½¢å®¹è¯æ€§åˆ«è¦ä¸åè¯ä¸€è‡´

## è¯­åºå’Œå¥æ³•é—®é¢˜
- ç¬¬1æ®µï¼šè¯­åºä¸è‡ªç„¶ â†’ å»ºè®®ï¼šè°ƒæ•´ä¸ºè¥¿ç­ç‰™è¯­æ ‡å‡†è¯­åº

## æ€§åˆ«å’Œæ•°çš„ä¸€è‡´æ€§
- "la problema" â†’ æ­£ç¡®å½¢å¼ï¼š"el problema" â†’ è¯´æ˜ï¼šproblemaæ˜¯é˜³æ€§è¯

## è¡¨è¾¾å»ºè®®
- å°è¯•ä½¿ç”¨æ›´å¤šçš„è¿æ¥è¯ä¸°å¯Œå¥å­ç»“æ„
- æ³¨æ„åŠ¨è¯æ—¶æ€çš„ç»Ÿä¸€æ€§

## å†…å®¹ç»“æ„å»ºè®®
- æ®µè½ä¹‹é—´ç¼ºä¹é€»è¾‘è¿æ¥
- ç»“å°¾å¯ä»¥æ›´å¥½åœ°å‘¼åº”å¼€å¤´

ã€å­¦ç”Ÿä½œæ–‡ã€‘
{text}

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼æ‰¹æ”¹ï¼š"""


def ai_correct_essay_stream(text: str, word_count: str = "ä¸é™å­—æ•°", grade: str = "ä¸‰å¹´çº§", language: str = "zh"):
    """
    ä½¿ç”¨AIæ‰¹æ”¹ä½œæ–‡çš„æµå¼ç‰ˆæœ¬ï¼Œå®æ—¶è¾“å‡ºæ€è€ƒè¿‡ç¨‹

    Args:
        text (str): è¦æ‰¹æ”¹çš„ä½œæ–‡å†…å®¹
        word_count (str): ä½œæ–‡å­—æ•°è¦æ±‚
        grade (str): å¹´çº§
        language (str): è¯­è¨€ä»£ç  "zh", "en", "es"

    Yields:
        Dict[str, Any]: åŒ…å«æµå¼è¾“å‡ºçš„å­—å…¸
            - type: 'thinking' | 'result' | 'error'
            - content: æ€è€ƒå†…å®¹ï¼ˆä»…å½“type='thinking'æ—¶ï¼‰
            - corrections: ä¿®æ”¹å»ºè®®åˆ—è¡¨ï¼ˆä»…å½“type='result'æ—¶ï¼‰
            - error: é”™è¯¯ä¿¡æ¯ï¼ˆä»…å½“type='error'æ—¶ï¼‰
    """
    if not text or not text.strip():
        yield {
            "type": "error",
            "error": "ä½œæ–‡å†…å®¹ä¸èƒ½ä¸ºç©º"
        }
        return

    try:
        # æ™ºèƒ½é€‰æ‹©LLMæä¾›è€…
        provider, model = get_smart_llm_provider(language)
        client = provider.get_llm(timeout=180)  # å¢åŠ åˆ°3åˆ†é’Ÿè¶…æ—¶
        
        # æ ¹æ®è¯­è¨€ç”Ÿæˆæ‰¹æ”¹æç¤ºè¯
        prompt = get_correction_prompt(text, language, word_count, grade)

        # æ˜¾ç¤ºä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯
        provider, model = get_smart_llm_provider(language)
        model_info = f"ä½¿ç”¨æ¨¡å‹ï¼š{model}"
        yield {
            "type": "thinking",
            "content": f"æ­£åœ¨è¿æ¥AIæ¨¡å‹...ï¼ˆ{model_info}ï¼‰"
        }
        
        # ä½¿ç”¨æµå¼è¾“å‡ºè°ƒç”¨AIæ¨¡å‹ï¼Œå¢åŠ é”™è¯¯é‡è¯•æœºåˆ¶
        max_retries = 3
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                stream = client.chat.completions.create(
                    model=model,
                    messages=[
                        {"role": "system", "content": get_system_prompt(language)},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    max_tokens=3000,  # å¢åŠ tokené™åˆ¶
                    stream=True,
                    timeout=180  # 3åˆ†é’Ÿè¶…æ—¶
                )
                break  # æˆåŠŸåˆ›å»ºï¼Œé€€å‡ºé‡è¯•å¾ªç¯
            except Exception as e:
                retry_count += 1
                if retry_count >= max_retries:
                    raise e
                yield {
                    "type": "thinking",
                    "content": f"è¿æ¥è¶…æ—¶ï¼Œæ­£åœ¨é‡è¯• ({retry_count}/{max_retries})..."
                }
                time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
        
        yield {
            "type": "thinking",
            "content": "AIå¼€å§‹æ€è€ƒæ‰¹æ”¹æ–¹æ¡ˆ..."
        }
        
        # æ”¶é›†æµå¼å“åº”ï¼Œå¢åŠ è¶…æ—¶å’Œå¿ƒè·³æ£€æµ‹
        full_response = ""
        current_line = ""
        last_chunk_time = time.time()
        chunk_timeout = 30  # 30ç§’å†…æ²¡æœ‰æ–°chunkåˆ™è®¤ä¸ºè¶…æ—¶
        
        try:
            for chunk in stream:
                current_time = time.time()
                
                # æ£€æŸ¥chunkè¶…æ—¶
                if current_time - last_chunk_time > chunk_timeout:
                    yield {
                        "type": "thinking",
                        "content": "âš ï¸ æ•°æ®ä¼ è¾“ç¼“æ…¢ï¼Œæ­£åœ¨ç­‰å¾…AIå“åº”..."
                    }
                
                last_chunk_time = current_time
                
                if chunk.choices[0].delta.content is not None:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    current_line += content
                    
                    # å®æ—¶æ˜¾ç¤ºAIçš„æ€è€ƒå†…å®¹
                    if content:
                        yield {
                            "type": "thinking",
                            "content": content
                        }
                    
                    # å½“é‡åˆ°æ¢è¡Œç¬¦æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´çš„æ®µè½æ ‡é¢˜
                    if "\n" in current_line:
                        lines = current_line.split("\n")
                        for line in lines[:-1]:  # å¤„ç†é™¤æœ€åä¸€è¡Œå¤–çš„æ‰€æœ‰è¡Œ
                            if line.strip().startswith("##"):
                                section_name = line.strip().replace("##", "").strip()
                                if section_name:
                                    yield {
                                        "type": "thinking",
                                        "content": f"\nğŸ“‹ å¼€å§‹åˆ†æï¼š{section_name}\n"
                                    }
                        current_line = lines[-1]  # ä¿ç•™æœ€åä¸€è¡Œç»§ç»­å¤„ç†
                        
        except Exception as stream_error:
            yield {
                "type": "thinking",
                "content": f"æµå¼ä¼ è¾“ä¸­æ–­: {str(stream_error)}ï¼Œæ­£åœ¨å¤„ç†å·²æ¥æ”¶çš„å†…å®¹..."
            }
            # ç»§ç»­å¤„ç†å·²æ¥æ”¶çš„å†…å®¹ï¼Œä¸ç›´æ¥æŠ›å‡ºå¼‚å¸¸
        
        yield {
            "type": "thinking",
            "content": "AIåˆ†æå®Œæˆï¼Œæ­£åœ¨æ•´ç†æ‰¹æ”¹ç»“æœ..."
        }
        
        # è§£æAIè¿”å›çš„æ‰¹æ”¹ç»“æœ
        corrections = parse_correction_response(full_response)
        
        # å‘é€å®Œæˆä¿¡å·
        yield {
            "type": "thinking",
            "content": "\nâœ… AIæ‰¹æ”¹å®Œæˆï¼\n"
        }
        
        yield {
            "type": "result",
            "corrections": corrections,
            "raw_response": full_response
        }
        
    except Exception as e:
        import traceback
        error_msg = str(e)
        
        # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
        if "timeout" in error_msg.lower():
            yield {
                "type": "error", 
                "error": "AIæœåŠ¡å“åº”è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•ã€‚å»ºè®®ï¼š1) æ£€æŸ¥ç½‘ç»œè¿æ¥ 2) ç¼©çŸ­ä½œæ–‡é•¿åº¦ 3) ç¨åå†è¯•"
            }
        elif "connection" in error_msg.lower():
            yield {
                "type": "error",
                "error": "ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œè¯·æ£€æŸ¥ç½‘ç»œçŠ¶æ€åé‡è¯•"
            }
        elif "rate limit" in error_msg.lower():
            yield {
                "type": "error",
                "error": "è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨ç­‰ç‰‡åˆ»åé‡è¯•"
            }
        else:
            yield {
                "type": "error",
                "error": f"AIæ‰¹æ”¹å¤±è´¥ï¼š{error_msg}"
            }
        
        # è®°å½•è¯¦ç»†é”™è¯¯æ—¥å¿—ï¼ˆä»…åœ¨å¼€å‘ç¯å¢ƒï¼‰
        import os
        if os.getenv('DEBUG') == 'True':
            print(f"Stream error traceback: {traceback.format_exc()}")


def parse_correction_response(response: str) -> List[Dict[str, Any]]:
    """
    è§£æAIæ‰¹æ”¹å“åº”ï¼Œæå–å„ç±»ä¿®æ”¹å»ºè®®
    
    Args:
        response (str): AIçš„æ‰¹æ”¹å“åº”
        
    Returns:
        List[Dict[str, Any]]: è§£æåçš„ä¿®æ”¹å»ºè®®åˆ—è¡¨
    """
    corrections = []
    
    # æŒ‰ç…§ä¸åŒçš„ä¿®æ”¹ç±»å‹åˆ†å‰²å“åº”ï¼Œä½¿ç”¨æ›´çµæ´»çš„åŒ¹é…
    sections = {
        "æ€»ä½“è¯„ä»·": [],
        "ç—…å¥ä¿®æ”¹": [],
        "é”™åˆ«å­—ä¿®æ”¹": [],
        "æ ‡ç‚¹ç¬¦å·ä¿®æ”¹": [],
        "è¯­è¨€è¡¨è¾¾æ”¹è¿›å»ºè®®": [],
        "å†…å®¹ç»“æ„æ”¹è¿›å»ºè®®": []
    }

    # å…³é”®è¯æ˜ å°„ï¼Œç”¨äºæ›´æ™ºèƒ½çš„åŒ¹é…ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
    section_keywords = {
        "æ€»ä½“è¯„ä»·": ["æ€»ä½“è¯„ä»·", "æ€»ä½“å°è±¡", "è¯„ä»·", "æ€»ç»“", "Overall Evaluation", "General Assessment", "Overall", "Evaluation"],
        "ç—…å¥ä¿®æ”¹": ["ç—…å¥ä¿®æ”¹", "ç—…å¥", "è¯­æ³•é”™è¯¯", "å¥å­é—®é¢˜", "è¯­æ³•é”™è¯¯ä¿®æ­£", "è¯­åºå’Œå¥æ³•é—®é¢˜", "å¥å­ç»“æ„é—®é¢˜", "Grammar Corrections", "Grammar", "Sentence Issues", "Sentence Structure"],
        "é”™åˆ«å­—ä¿®æ”¹": ["é”™åˆ«å­—ä¿®æ”¹", "é”™åˆ«å­—", "é”™å­—", "åˆ«å­—", "å­—è¯é”™è¯¯", "è¯æ±‡ä½¿ç”¨æ”¹è¿›", "æ€§åˆ«å’Œæ•°çš„ä¸€è‡´æ€§", "Vocabulary Improvements", "Vocabulary", "Word Choice"],
        "æ ‡ç‚¹ç¬¦å·ä¿®æ”¹": ["æ ‡ç‚¹ç¬¦å·ä¿®æ”¹", "æ ‡ç‚¹ç¬¦å·", "æ ‡ç‚¹", "ç¬¦å·", "Punctuation", "Punctuation Corrections"],
        "è¯­è¨€è¡¨è¾¾æ”¹è¿›å»ºè®®": ["è¯­è¨€è¡¨è¾¾æ”¹è¿›å»ºè®®", "è¯­è¨€è¡¨è¾¾", "è¡¨è¾¾å»ºè®®", "è¯­è¨€æ”¹è¿›", "ä¸­å¼è‹±è¯­ä¿®æ­£", "è¡¨è¾¾å»ºè®®", "Language Expression Tips", "Expression", "Language Tips", "Natural English"],
        "å†…å®¹ç»“æ„æ”¹è¿›å»ºè®®": ["å†…å®¹ç»“æ„æ”¹è¿›å»ºè®®", "å†…å®¹ç»“æ„", "ç»“æ„å»ºè®®", "å†…å®¹æ”¹è¿›", "å†…å®¹ç»“æ„å»ºè®®", "Content & Organization Tips", "Organization", "Structure", "Content Tips"]
    }
    
    current_section = None
    lines = response.split('\n')
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„åˆ†ç±»æ ‡é¢˜ - æ›´æ™ºèƒ½çš„åŒ¹é…
        if line.startswith('##') or line.startswith('#'):
            section_title = line.replace('##', '').replace('#', '').strip()

            # ç§»é™¤å¯èƒ½çš„è£…é¥°ç¬¦å· **bold** ç­‰
            import re
            section_title_clean = re.sub(r'\*\*([^*]+)\*\*', r'\1', section_title)
            section_title_clean = section_title_clean.strip()

            # å°è¯•ç›´æ¥åŒ¹é…
            if section_title_clean in sections:
                current_section = section_title_clean
                continue

            # å°è¯•å…³é”®è¯åŒ¹é…ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
            for section_name, keywords in section_keywords.items():
                if any(keyword.lower() in section_title_clean.lower() for keyword in keywords):
                    current_section = section_name
                    break
            continue
        
        # å¦‚æœæ˜¯åˆ—è¡¨é¡¹ï¼Œæ·»åŠ åˆ°å½“å‰åˆ†ç±»
        if (line.startswith('-') or line.startswith('â€¢') or line.startswith('*') or line.startswith('â€“')) and current_section:
            correction_text = line[1:].strip()
            if correction_text:
                sections[current_section].append(correction_text)
        elif current_section and line and not line.startswith('ã€') and not line.startswith('##'):
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„åˆ—è¡¨æ ‡è®°ï¼Œä½†æœ‰å†…å®¹ä¸”åœ¨æŸä¸ªsectionä¸­ï¼Œä¹Ÿæ·»åŠ è¿›å»
            # ä½†æ’é™¤ä»¥ä¸‹æƒ…å†µï¼šæ ‡é¢˜è¡Œã€ç©ºè¡Œã€ç‰¹æ®Šæ ‡è®°è¡Œ
            if line.strip() and not line.startswith('è¯·') and not line.startswith('æ³¨æ„'):
                sections[current_section].append(line)
    
    # å°†å„ä¸ªåˆ†ç±»çš„å†…å®¹æ•´ç†æˆæœ€ç»ˆæ ¼å¼
    for section_name, items in sections.items():
        if items:
            corrections.append({
                "type": section_name,
                "items": items
            })
    
    # å¦‚æœæ²¡æœ‰è§£æåˆ°ä»»ä½•å†…å®¹ï¼Œå°è¯•å°†æ•´ä¸ªå“åº”ä½œä¸ºæ€»ä½“è¯„ä»·
    if not corrections and response.strip():
        corrections.append({
            "type": "æ€»ä½“è¯„ä»·",
            "items": [response.strip()]
        })
    
    return corrections


if __name__ == "__main__":
    # æµ‹è¯•å‡½æ•°
    test_text = "ä½ å¥½ï¼Œä¸–ç•Œï¼Hello, world! è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ã€‚This is a test."
    result = analyze_text(test_text)
    print(format_analysis_result(result)) 