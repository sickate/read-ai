import re
from typing import Dict, Any, List
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥appæ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.llm.providers import get_provider_config, AliyunModel


def analyze_text(text: str) -> Dict[str, Any]:
    """
    åˆ†ææ–‡æœ¬ï¼Œç»Ÿè®¡æ±‰å­—å­—æ•°ã€æ ‡ç‚¹ç¬¦å·æ•°å’Œè‹±æ–‡å•è¯æ•°
    
    Args:
        text (str): è¦åˆ†æçš„æ–‡æœ¬
        
    Returns:
        Dict[str, Any]: åŒ…å«ç»Ÿè®¡ç»“æœçš„å­—å…¸
            - chinese_chars: æ±‰å­—å­—æ•°
            - punctuation: æ ‡ç‚¹ç¬¦å·æ•°ï¼ˆåŒ…æ‹¬å…¨è§’å’ŒåŠè§’ï¼‰
            - english_words: è‹±æ–‡å•è¯æ•°
            - total_chars: æ€»å­—ç¬¦æ•°
    """
    if not text:
        return {
            "chinese_chars": 0,
            "punctuation": 0,
            "english_words": 0,
            "total_chars": 0
        }
    
    # ç»Ÿè®¡æ±‰å­— (ä½¿ç”¨å¸¸ç”¨æ±‰å­—çš„UnicodeèŒƒå›´)
    # \u4e00-\u9fff: CJKç»Ÿä¸€æ±‰å­—åŸºæœ¬åŒºåŸŸ
    chinese_pattern = r'[\u4e00-\u9fff]'
    chinese_chars = len(re.findall(chinese_pattern, text))
    
    # ç»Ÿè®¡æ ‡ç‚¹ç¬¦å· (åŒ…æ‹¬ä¸­æ–‡å’Œè‹±æ–‡æ ‡ç‚¹)
    # ä¸­æ–‡æ ‡ç‚¹ç¬¦å·å’Œå…¨è§’ç¬¦å·
    chinese_punctuation_pattern = r'[\u3000-\u303f\uff00-\uffef]'
    # è‹±æ–‡æ ‡ç‚¹ç¬¦å·
    english_punctuation_pattern = r'[!\"#$%&\'()*+,\-./:;<=>?@\[\\\]^_`{|}~]'
    
    # åˆ†åˆ«ç»Ÿè®¡ä¸­è‹±æ–‡æ ‡ç‚¹
    chinese_punctuation_count = len(re.findall(chinese_punctuation_pattern, text))
    english_punctuation_count = len(re.findall(english_punctuation_pattern, text))
    punctuation = chinese_punctuation_count + english_punctuation_count
    
    # ç»Ÿè®¡è‹±æ–‡å•è¯æ•°
    # åŒ¹é…è‹±æ–‡å•è¯ (è¿ç»­çš„å­—æ¯ï¼Œå¯èƒ½åŒ…å«æ’‡å·)
    english_word_pattern = r"[a-zA-Z]+(?:'[a-zA-Z]+)?"
    english_words = len(re.findall(english_word_pattern, text))
    
    # æ€»å­—ç¬¦æ•°
    total_chars = len(text)
    
    return {
        "chinese_chars": chinese_chars,
        "punctuation": punctuation,
        "english_words": english_words,
        "total_chars": total_chars
    }


def format_analysis_result(result: Dict[str, Any]) -> str:
    """
    æ ¼å¼åŒ–åˆ†æç»“æœä¸ºå¯è¯»å­—ç¬¦ä¸²
    
    Args:
        result (Dict[str, Any]): analyze_textå‡½æ•°çš„è¿”å›ç»“æœ
        
    Returns:
        str: æ ¼å¼åŒ–çš„ç»“æœå­—ç¬¦ä¸²
    """
    return f"""
ğŸ“Š æ–‡æœ¬åˆ†æç»“æœ:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ æ±‰å­—å­—æ•°:     {result['chinese_chars']:,}
ğŸ”¤ è‹±æ–‡å•è¯æ•°:   {result['english_words']:,}
ğŸ”£ æ ‡ç‚¹ç¬¦å·æ•°:   {result['punctuation']:,}
ğŸ“„ æ€»å­—ç¬¦æ•°:     {result['total_chars']:,}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""


def ai_correct_essay(text: str, word_count: str = "ä¸é™å­—æ•°", grade: str = "ä¸‰å¹´çº§") -> Dict[str, Any]:
    """
    ä½¿ç”¨AIæ‰¹æ”¹ä½œæ–‡ï¼Œæ£€æŸ¥è¯­æ³•ã€é”™åˆ«å­—ã€æ ‡ç‚¹ç¬¦å·ç­‰é—®é¢˜
    
    Args:
        text (str): è¦æ‰¹æ”¹çš„ä½œæ–‡å†…å®¹
        word_count (str): ä½œæ–‡å­—æ•°è¦æ±‚
        grade (str): å¹´çº§
        
    Returns:
        Dict[str, Any]: åŒ…å«æ‰¹æ”¹ç»“æœçš„å­—å…¸
            - success: æ˜¯å¦æˆåŠŸ
            - corrections: ä¿®æ”¹å»ºè®®åˆ—è¡¨
            - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    """
    if not text or not text.strip():
        return {
            "success": False,
            "error": "ä½œæ–‡å†…å®¹ä¸èƒ½ä¸ºç©º"
        }
    
    try:
        # è·å–é˜¿é‡Œäº‘é…ç½®
        provider = get_provider_config('aliyun')
        client = provider.get_llm()
        
        # æ„å»ºæ‰¹æ”¹æç¤ºè¯
        grade_requirements = {
            "ä¸€å¹´çº§": "ç®€å•è¯­å¥ï¼ŒåŸºæœ¬è¡¨è¾¾æ¸…æ¥š",
            "äºŒå¹´çº§": "å¥å­é€šé¡ºï¼Œæœ‰åŸºæœ¬çš„æ®µè½æ¦‚å¿µ",
            "ä¸‰å¹´çº§": "è¯­å¥é€šé¡ºï¼Œæœ‰å¼€å¤´ã€ä¸­é—´ã€ç»“å°¾",
            "å››å¹´çº§": "ç»“æ„æ¸…æ¥šï¼Œè¯­è¨€è¾ƒä¸ºæµç•…",
            "äº”å¹´çº§": "å†…å®¹å……å®ï¼Œè¯­è¨€ç”ŸåŠ¨",
            "å…­å¹´çº§": "æ–‡ç« ç»“æ„å®Œæ•´ï¼Œè¯­è¨€å‡†ç¡®ç”ŸåŠ¨",
            "åˆä¸€": "è§‚ç‚¹æ˜ç¡®ï¼Œè®ºè¯æœ‰æ¡ç†",
            "åˆäºŒ": "è¯­è¨€è¡¨è¾¾å‡†ç¡®ï¼Œæœ‰ä¸€å®šæ–‡é‡‡",
            "åˆä¸‰": "æ€æƒ³æ·±åˆ»ï¼Œè¯­è¨€ä¼˜ç¾ï¼Œç»“æ„ä¸¥è°¨"
        }
        
        grade_req = grade_requirements.get(grade, "è¯­å¥é€šé¡ºï¼Œå†…å®¹å®Œæ•´")
        word_count_req = f"å­—æ•°è¦æ±‚ï¼š{word_count}" if word_count != "ä¸é™å­—æ•°" else "å­—æ•°æ— ç‰¹æ®Šè¦æ±‚"
        
        prompt = f"""è¯·ä½ ä½œä¸ºä¸€åä¸“ä¸šçš„è¯­æ–‡è€å¸ˆï¼Œä»”ç»†æ‰¹æ”¹ä»¥ä¸‹{grade}å­¦ç”Ÿçš„ä½œæ–‡ã€‚

ã€ä½œæ–‡è¦æ±‚ã€‘
- å¹´çº§ï¼š{grade}
- {word_count_req}
- è¯„åˆ¤æ ‡å‡†ï¼š{grade_req}

ã€æ‰¹æ”¹è¦æ±‚ã€‘
1. ä¸è¦ä¿®æ”¹åŸæ–‡ï¼Œåªæ ‡å‡ºéœ€è¦ä¿®æ”¹çš„åœ°æ–¹
2. è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½è¦åŒ…å«ï¼š

## æ€»ä½“è¯„ä»·
- æ€»ä½“å°è±¡ï¼šï¼ˆå¯¹æ•´ç¯‡ä½œæ–‡çš„æ€»ä½“è¯„ä»·ï¼‰
- ä¼˜ç‚¹ï¼šï¼ˆåˆ—å‡ºä½œæ–‡çš„ä¼˜ç‚¹ï¼‰
- ä¸»è¦é—®é¢˜ï¼šï¼ˆåˆ—å‡ºä¸»è¦éœ€è¦æ”¹è¿›çš„é—®é¢˜ï¼‰
- å»ºè®®å¾—åˆ†ï¼šï¼ˆæŒ‰{grade}æ ‡å‡†ç»™å‡ºå»ºè®®åˆ†æ•°ï¼Œæ»¡åˆ†100åˆ†ï¼‰

## ç—…å¥ä¿®æ”¹
- ç¬¬Xæ®µç¬¬Yå¥ï¼š"åŸæ–‡å†…å®¹" â†’ é—®é¢˜ï¼šå…·ä½“é—®é¢˜æè¿° â†’ å»ºè®®ï¼šå¦‚ä½•ä¿®æ”¹çš„å»ºè®®

## é”™åˆ«å­—ä¿®æ”¹  
- ç¬¬Xæ®µï¼š"é”™å­—" â†’ åº”ä¸ºï¼š"æ­£å­—" â†’ ä½ç½®ï¼šå…·ä½“ä½ç½®æè¿°

## æ ‡ç‚¹ç¬¦å·ä¿®æ”¹
- ç¬¬Xæ®µï¼šé—®é¢˜æè¿° â†’ å»ºè®®ï¼šæ­£ç¡®ç”¨æ³•

## è¯­è¨€è¡¨è¾¾æ”¹è¿›å»ºè®®
- å»ºè®®å†…å®¹ï¼ˆç»™å‡ºå»ºè®®å’Œä¾‹å­ï¼Œä½†ä¸ç›´æ¥æä¾›ä¿®æ”¹åçš„åŸæ–‡ï¼‰

## å†…å®¹ç»“æ„æ”¹è¿›å»ºè®®
- å»ºè®®å†…å®¹

ã€ä½œæ–‡å†…å®¹ã€‘
{text}

è¯·å¼€å§‹æ‰¹æ”¹ï¼š"""

        # è°ƒç”¨AIæ¨¡å‹
        response = client.chat.completions.create(
            model=AliyunModel.DEEPSEEK_R1.value,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€åä¸“ä¸šçš„è¯­æ–‡è€å¸ˆï¼Œè´Ÿè´£æ‰¹æ”¹å­¦ç”Ÿä½œæ–‡ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=2000
        )
        
        ai_response = response.choices[0].message.content
        
        # è§£æAIè¿”å›çš„æ‰¹æ”¹ç»“æœ
        corrections = parse_correction_response(ai_response)
        
        return {
            "success": True,
            "corrections": corrections,
            "raw_response": ai_response
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"AIæ‰¹æ”¹å¤±è´¥ï¼š{str(e)}"
        }


def ai_correct_essay_stream(text: str, word_count: str = "ä¸é™å­—æ•°", grade: str = "ä¸‰å¹´çº§"):
    """
    ä½¿ç”¨AIæ‰¹æ”¹ä½œæ–‡çš„æµå¼ç‰ˆæœ¬ï¼Œå®æ—¶è¾“å‡ºæ€è€ƒè¿‡ç¨‹
    
    Args:
        text (str): è¦æ‰¹æ”¹çš„ä½œæ–‡å†…å®¹
        word_count (str): ä½œæ–‡å­—æ•°è¦æ±‚
        grade (str): å¹´çº§
        
    Yields:
        Dict[str, Any]: åŒ…å«æµå¼è¾“å‡ºçš„å­—å…¸
            - type: 'thinking' | 'result' | 'error'
            - content: æ€è€ƒå†…å®¹ï¼ˆä»…å½“type='thinking'æ—¶ï¼‰
            - corrections: ä¿®æ”¹å»ºè®®åˆ—è¡¨ï¼ˆä»…å½“type='result'æ—¶ï¼‰
            - error: é”™è¯¯ä¿¡æ¯ï¼ˆä»…å½“type='error'æ—¶ï¼‰
    """
    if not text or not text.strip():
        yield {
            "type": "error",
            "error": "ä½œæ–‡å†…å®¹ä¸èƒ½ä¸ºç©º"
        }
        return
    
    try:
        # è·å–é˜¿é‡Œäº‘é…ç½®
        provider = get_provider_config('aliyun')
        client = provider.get_llm()
        
        # æ„å»ºæ‰¹æ”¹æç¤ºè¯
        grade_requirements = {
            "ä¸€å¹´çº§": "ç®€å•è¯­å¥ï¼ŒåŸºæœ¬è¡¨è¾¾æ¸…æ¥š",
            "äºŒå¹´çº§": "å¥å­é€šé¡ºï¼Œæœ‰åŸºæœ¬çš„æ®µè½æ¦‚å¿µ",
            "ä¸‰å¹´çº§": "è¯­å¥é€šé¡ºï¼Œæœ‰å¼€å¤´ã€ä¸­é—´ã€ç»“å°¾",
            "å››å¹´çº§": "ç»“æ„æ¸…æ¥šï¼Œè¯­è¨€è¾ƒä¸ºæµç•…",
            "äº”å¹´çº§": "å†…å®¹å……å®ï¼Œè¯­è¨€ç”ŸåŠ¨",
            "å…­å¹´çº§": "æ–‡ç« ç»“æ„å®Œæ•´ï¼Œè¯­è¨€å‡†ç¡®ç”ŸåŠ¨",
            "åˆä¸€": "è§‚ç‚¹æ˜ç¡®ï¼Œè®ºè¯æœ‰æ¡ç†",
            "åˆäºŒ": "è¯­è¨€è¡¨è¾¾å‡†ç¡®ï¼Œæœ‰ä¸€å®šæ–‡é‡‡",
            "åˆä¸‰": "æ€æƒ³æ·±åˆ»ï¼Œè¯­è¨€ä¼˜ç¾ï¼Œç»“æ„ä¸¥è°¨"
        }
        
        grade_req = grade_requirements.get(grade, "è¯­å¥é€šé¡ºï¼Œå†…å®¹å®Œæ•´")
        word_count_req = f"å­—æ•°è¦æ±‚ï¼š{word_count}" if word_count != "ä¸é™å­—æ•°" else "å­—æ•°æ— ç‰¹æ®Šè¦æ±‚"
        
        prompt = f"""è¯·ä½ ä½œä¸ºä¸€åä¸“ä¸šçš„è¯­æ–‡è€å¸ˆï¼Œä»”ç»†æ‰¹æ”¹ä»¥ä¸‹{grade}å­¦ç”Ÿçš„ä½œæ–‡ã€‚

ã€ä½œæ–‡è¦æ±‚ã€‘
- å¹´çº§ï¼š{grade}
- {word_count_req}
- è¯„åˆ¤æ ‡å‡†ï¼š{grade_req}

ã€æ‰¹æ”¹è¦æ±‚ã€‘
1. ä¸è¦ä¿®æ”¹åŸæ–‡ï¼Œåªæ ‡å‡ºéœ€è¦ä¿®æ”¹çš„åœ°æ–¹
2. è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½è¦åŒ…å«ï¼š

## æ€»ä½“è¯„ä»·
- æ€»ä½“å°è±¡ï¼šï¼ˆå¯¹æ•´ç¯‡ä½œæ–‡çš„æ€»ä½“è¯„ä»·ï¼‰
- ä¼˜ç‚¹ï¼šï¼ˆåˆ—å‡ºä½œæ–‡çš„ä¼˜ç‚¹ï¼‰
- ä¸»è¦é—®é¢˜ï¼šï¼ˆåˆ—å‡ºä¸»è¦éœ€è¦æ”¹è¿›çš„é—®é¢˜ï¼‰
- å»ºè®®å¾—åˆ†ï¼šï¼ˆæŒ‰{grade}æ ‡å‡†ç»™å‡ºå»ºè®®åˆ†æ•°ï¼Œæ»¡åˆ†100åˆ†ï¼‰

## ç—…å¥ä¿®æ”¹
- ç¬¬Xæ®µç¬¬Yå¥ï¼š"åŸæ–‡å†…å®¹" â†’ é—®é¢˜ï¼šå…·ä½“é—®é¢˜æè¿° â†’ å»ºè®®ï¼šå¦‚ä½•ä¿®æ”¹çš„å»ºè®®

## é”™åˆ«å­—ä¿®æ”¹  
- ç¬¬Xæ®µï¼š"é”™å­—" â†’ åº”ä¸ºï¼š"æ­£å­—" â†’ ä½ç½®ï¼šå…·ä½“ä½ç½®æè¿°

## æ ‡ç‚¹ç¬¦å·ä¿®æ”¹
- ç¬¬Xæ®µï¼šé—®é¢˜æè¿° â†’ å»ºè®®ï¼šæ­£ç¡®ç”¨æ³•

## è¯­è¨€è¡¨è¾¾æ”¹è¿›å»ºè®®
- å»ºè®®å†…å®¹ï¼ˆç»™å‡ºå»ºè®®å’Œä¾‹å­ï¼Œä½†ä¸ç›´æ¥æä¾›ä¿®æ”¹åçš„åŸæ–‡ï¼‰

## å†…å®¹ç»“æ„æ”¹è¿›å»ºè®®
- å»ºè®®å†…å®¹

ã€ä½œæ–‡å†…å®¹ã€‘
{text}

è¯·å¼€å§‹æ‰¹æ”¹ï¼š"""

        yield {
            "type": "thinking",
            "content": "æ­£åœ¨è¿æ¥AIæ¨¡å‹..."
        }
        
        # ä½¿ç”¨æµå¼è¾“å‡ºè°ƒç”¨AIæ¨¡å‹
        stream = client.chat.completions.create(
            model=AliyunModel.DEEPSEEK_R1.value,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€åä¸“ä¸šçš„è¯­æ–‡è€å¸ˆï¼Œè´Ÿè´£æ‰¹æ”¹å­¦ç”Ÿä½œæ–‡ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=2000,
            stream=True
        )
        
        yield {
            "type": "thinking",
            "content": "AIå¼€å§‹æ€è€ƒæ‰¹æ”¹æ–¹æ¡ˆ..."
        }
        
        # æ”¶é›†æµå¼å“åº”
        full_response = ""
        current_line = ""
        
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                full_response += content
                current_line += content
                
                # å®æ—¶æ˜¾ç¤ºAIçš„æ€è€ƒå†…å®¹
                if content:
                    yield {
                        "type": "thinking",
                        "content": content
                    }
                
                # å½“é‡åˆ°æ¢è¡Œç¬¦æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´çš„æ®µè½æ ‡é¢˜
                if "\n" in current_line:
                    lines = current_line.split("\n")
                    for line in lines[:-1]:  # å¤„ç†é™¤æœ€åä¸€è¡Œå¤–çš„æ‰€æœ‰è¡Œ
                        if line.strip().startswith("##"):
                            section_name = line.strip().replace("##", "").strip()
                            if section_name:
                                yield {
                                    "type": "thinking",
                                    "content": f"\nğŸ“‹ å¼€å§‹åˆ†æï¼š{section_name}\n"
                                }
                    current_line = lines[-1]  # ä¿ç•™æœ€åä¸€è¡Œç»§ç»­å¤„ç†
        
        yield {
            "type": "thinking",
            "content": "AIåˆ†æå®Œæˆï¼Œæ­£åœ¨æ•´ç†æ‰¹æ”¹ç»“æœ..."
        }
        
        # è§£æAIè¿”å›çš„æ‰¹æ”¹ç»“æœ
        corrections = parse_correction_response(full_response)
        
        # å‘é€å®Œæˆä¿¡å·
        yield {
            "type": "thinking",
            "content": "\nâœ… AIæ‰¹æ”¹å®Œæˆï¼\n"
        }
        
        yield {
            "type": "result",
            "corrections": corrections,
            "raw_response": full_response
        }
        
    except Exception as e:
        yield {
            "type": "error",
            "error": f"AIæ‰¹æ”¹å¤±è´¥ï¼š{str(e)}"
        }


def parse_correction_response(response: str) -> List[Dict[str, Any]]:
    """
    è§£æAIæ‰¹æ”¹å“åº”ï¼Œæå–å„ç±»ä¿®æ”¹å»ºè®®
    
    Args:
        response (str): AIçš„æ‰¹æ”¹å“åº”
        
    Returns:
        List[Dict[str, Any]]: è§£æåçš„ä¿®æ”¹å»ºè®®åˆ—è¡¨
    """
    corrections = []
    
    # æŒ‰ç…§ä¸åŒçš„ä¿®æ”¹ç±»å‹åˆ†å‰²å“åº”ï¼Œä½¿ç”¨æ›´çµæ´»çš„åŒ¹é…
    sections = {
        "æ€»ä½“è¯„ä»·": [],
        "ç—…å¥ä¿®æ”¹": [],
        "é”™åˆ«å­—ä¿®æ”¹": [],
        "æ ‡ç‚¹ç¬¦å·ä¿®æ”¹": [],
        "è¯­è¨€è¡¨è¾¾æ”¹è¿›å»ºè®®": [],
        "å†…å®¹ç»“æ„æ”¹è¿›å»ºè®®": []
    }
    
    # å…³é”®è¯æ˜ å°„ï¼Œç”¨äºæ›´æ™ºèƒ½çš„åŒ¹é…
    section_keywords = {
        "æ€»ä½“è¯„ä»·": ["æ€»ä½“è¯„ä»·", "æ€»ä½“å°è±¡", "è¯„ä»·", "æ€»ç»“"],
        "ç—…å¥ä¿®æ”¹": ["ç—…å¥ä¿®æ”¹", "ç—…å¥", "è¯­æ³•é”™è¯¯", "å¥å­é—®é¢˜"],
        "é”™åˆ«å­—ä¿®æ”¹": ["é”™åˆ«å­—ä¿®æ”¹", "é”™åˆ«å­—", "é”™å­—", "åˆ«å­—", "å­—è¯é”™è¯¯"],
        "æ ‡ç‚¹ç¬¦å·ä¿®æ”¹": ["æ ‡ç‚¹ç¬¦å·ä¿®æ”¹", "æ ‡ç‚¹ç¬¦å·", "æ ‡ç‚¹", "ç¬¦å·"],
        "è¯­è¨€è¡¨è¾¾æ”¹è¿›å»ºè®®": ["è¯­è¨€è¡¨è¾¾æ”¹è¿›å»ºè®®", "è¯­è¨€è¡¨è¾¾", "è¡¨è¾¾å»ºè®®", "è¯­è¨€æ”¹è¿›"],
        "å†…å®¹ç»“æ„æ”¹è¿›å»ºè®®": ["å†…å®¹ç»“æ„æ”¹è¿›å»ºè®®", "å†…å®¹ç»“æ„", "ç»“æ„å»ºè®®", "å†…å®¹æ”¹è¿›"]
    }
    
    current_section = None
    lines = response.split('\n')
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„åˆ†ç±»æ ‡é¢˜ - æ›´æ™ºèƒ½çš„åŒ¹é…
        if line.startswith('##') or line.startswith('#'):
            section_title = line.replace('##', '').replace('#', '').strip()
            
            # å°è¯•ç›´æ¥åŒ¹é…
            if section_title in sections:
                current_section = section_title
                continue
            
            # å°è¯•å…³é”®è¯åŒ¹é…
            for section_name, keywords in section_keywords.items():
                if any(keyword in section_title for keyword in keywords):
                    current_section = section_name
                    break
            continue
        
        # å¦‚æœæ˜¯åˆ—è¡¨é¡¹ï¼Œæ·»åŠ åˆ°å½“å‰åˆ†ç±»
        if (line.startswith('-') or line.startswith('â€¢') or line.startswith('*')) and current_section:
            correction_text = line[1:].strip()
            if correction_text:
                sections[current_section].append(correction_text)
        elif current_section and line and not line.startswith('ã€'):
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„åˆ—è¡¨æ ‡è®°ï¼Œä½†æœ‰å†…å®¹ä¸”åœ¨æŸä¸ªsectionä¸­ï¼Œä¹Ÿæ·»åŠ è¿›å»
            sections[current_section].append(line)
    
    # å°†å„ä¸ªåˆ†ç±»çš„å†…å®¹æ•´ç†æˆæœ€ç»ˆæ ¼å¼
    for section_name, items in sections.items():
        if items:
            corrections.append({
                "type": section_name,
                "items": items
            })
    
    # å¦‚æœæ²¡æœ‰è§£æåˆ°ä»»ä½•å†…å®¹ï¼Œå°è¯•å°†æ•´ä¸ªå“åº”ä½œä¸ºæ€»ä½“è¯„ä»·
    if not corrections and response.strip():
        corrections.append({
            "type": "æ€»ä½“è¯„ä»·",
            "items": [response.strip()]
        })
    
    return corrections


if __name__ == "__main__":
    # æµ‹è¯•å‡½æ•°
    test_text = "ä½ å¥½ï¼Œä¸–ç•Œï¼Hello, world! è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ã€‚This is a test."
    result = analyze_text(test_text)
    print(format_analysis_result(result)) 