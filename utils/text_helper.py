import re
from typing import Dict, Any, List
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥appæ¨¡å—
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.llm.providers import get_provider_config, AliyunModel


def analyze_text(text: str) -> Dict[str, Any]:
    """
    åˆ†ææ–‡æœ¬ï¼Œç»Ÿè®¡æ±‰å­—å­—æ•°ã€æ ‡ç‚¹ç¬¦å·æ•°å’Œè‹±æ–‡å•è¯æ•°
    
    Args:
        text (str): è¦åˆ†æçš„æ–‡æœ¬
        
    Returns:
        Dict[str, Any]: åŒ…å«ç»Ÿè®¡ç»“æœçš„å­—å…¸
            - chinese_chars: æ±‰å­—å­—æ•°
            - punctuation: æ ‡ç‚¹ç¬¦å·æ•°ï¼ˆåŒ…æ‹¬å…¨è§’å’ŒåŠè§’ï¼‰
            - english_words: è‹±æ–‡å•è¯æ•°
            - total_chars: æ€»å­—ç¬¦æ•°
    """
    if not text:
        return {
            "chinese_chars": 0,
            "punctuation": 0,
            "english_words": 0,
            "total_chars": 0
        }
    
    # ç»Ÿè®¡æ±‰å­— (ä½¿ç”¨å¸¸ç”¨æ±‰å­—çš„UnicodeèŒƒå›´)
    # \u4e00-\u9fff: CJKç»Ÿä¸€æ±‰å­—åŸºæœ¬åŒºåŸŸ
    chinese_pattern = r'[\u4e00-\u9fff]'
    chinese_chars = len(re.findall(chinese_pattern, text))
    
    # ç»Ÿè®¡æ ‡ç‚¹ç¬¦å· (åŒ…æ‹¬ä¸­æ–‡å’Œè‹±æ–‡æ ‡ç‚¹)
    # ä¸­æ–‡æ ‡ç‚¹ç¬¦å·å’Œå…¨è§’ç¬¦å·
    chinese_punctuation_pattern = r'[\u3000-\u303f\uff00-\uffef]'
    # è‹±æ–‡æ ‡ç‚¹ç¬¦å·
    english_punctuation_pattern = r'[!\"#$%&\'()*+,\-./:;<=>?@\[\\\]^_`{|}~]'
    
    # åˆ†åˆ«ç»Ÿè®¡ä¸­è‹±æ–‡æ ‡ç‚¹
    chinese_punctuation_count = len(re.findall(chinese_punctuation_pattern, text))
    english_punctuation_count = len(re.findall(english_punctuation_pattern, text))
    punctuation = chinese_punctuation_count + english_punctuation_count
    
    # ç»Ÿè®¡è‹±æ–‡å•è¯æ•°
    # åŒ¹é…è‹±æ–‡å•è¯ (è¿ç»­çš„å­—æ¯ï¼Œå¯èƒ½åŒ…å«æ’‡å·)
    english_word_pattern = r"[a-zA-Z]+(?:'[a-zA-Z]+)?"
    english_words = len(re.findall(english_word_pattern, text))
    
    # æ€»å­—ç¬¦æ•°
    total_chars = len(text)
    
    return {
        "chinese_chars": chinese_chars,
        "punctuation": punctuation,
        "english_words": english_words,
        "total_chars": total_chars
    }


def format_analysis_result(result: Dict[str, Any]) -> str:
    """
    æ ¼å¼åŒ–åˆ†æç»“æœä¸ºå¯è¯»å­—ç¬¦ä¸²
    
    Args:
        result (Dict[str, Any]): analyze_textå‡½æ•°çš„è¿”å›ç»“æœ
        
    Returns:
        str: æ ¼å¼åŒ–çš„ç»“æœå­—ç¬¦ä¸²
    """
    return f"""
ğŸ“Š æ–‡æœ¬åˆ†æç»“æœ:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“ æ±‰å­—å­—æ•°:     {result['chinese_chars']:,}
ğŸ”¤ è‹±æ–‡å•è¯æ•°:   {result['english_words']:,}
ğŸ”£ æ ‡ç‚¹ç¬¦å·æ•°:   {result['punctuation']:,}
ğŸ“„ æ€»å­—ç¬¦æ•°:     {result['total_chars']:,}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""


def ai_correct_essay(text: str) -> Dict[str, Any]:
    """
    ä½¿ç”¨AIæ‰¹æ”¹ä½œæ–‡ï¼Œæ£€æŸ¥è¯­æ³•ã€é”™åˆ«å­—ã€æ ‡ç‚¹ç¬¦å·ç­‰é—®é¢˜
    
    Args:
        text (str): è¦æ‰¹æ”¹çš„ä½œæ–‡å†…å®¹
        
    Returns:
        Dict[str, Any]: åŒ…å«æ‰¹æ”¹ç»“æœçš„å­—å…¸
            - success: æ˜¯å¦æˆåŠŸ
            - corrections: ä¿®æ”¹å»ºè®®åˆ—è¡¨
            - error: é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    """
    if not text or not text.strip():
        return {
            "success": False,
            "error": "ä½œæ–‡å†…å®¹ä¸èƒ½ä¸ºç©º"
        }
    
    try:
        # è·å–é˜¿é‡Œäº‘é…ç½®
        provider = get_provider_config('aliyun')
        client = provider.get_llm()
        
        # æ„å»ºæ‰¹æ”¹æç¤ºè¯
        prompt = f"""è¯·ä½ ä½œä¸ºä¸€åä¸“ä¸šçš„è¯­æ–‡è€å¸ˆï¼Œä»”ç»†æ‰¹æ”¹ä»¥ä¸‹ä½œæ–‡ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚ï¼š

1. ä¸è¦ä¿®æ”¹åŸæ–‡ï¼Œåªæ ‡å‡ºéœ€è¦ä¿®æ”¹çš„åœ°æ–¹
2. ç”¨æ¡ç›®çš„æ–¹å¼åˆ—å‡ºä¿®æ”¹æ„è§ï¼Œéœ€è¦åŒ…æ‹¬ï¼š
   - ç—…å¥ï¼ˆè¯­æ³•é”™è¯¯ã€è¡¨è¾¾ä¸å½“ï¼‰
   - é”™åˆ«å­—ï¼ˆé”™å­—ã€åˆ«å­—ï¼‰
   - æ ‡ç‚¹ç¬¦å·é”™è¯¯
   - è¯­è¨€è¡¨è¾¾æ–¹é¢çš„æ”¹è¿›å»ºè®®
   - å†…å®¹ç»“æ„æ–¹é¢çš„æ”¹è¿›å»ºè®®

3. å¯¹äºè¯­è¨€ã€å†…å®¹æ–¹é¢çš„ä¿®æ”¹ï¼Œç»™å‡ºä¿®æ”¹æ„è§å’Œä¾‹å­ï¼Œä½†ä¸è¦ç»™å‡ºå¯ä»¥ç›´æ¥ä½¿ç”¨çš„ä¿®æ”¹ååŸæ–‡
4. è¯·ç”¨ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

## ç—…å¥ä¿®æ”¹
- ç¬¬Xæ®µç¬¬Yå¥ï¼š"åŸæ–‡å†…å®¹" â†’ é—®é¢˜ï¼šå…·ä½“é—®é¢˜æè¿° â†’ å»ºè®®ï¼šå¦‚ä½•ä¿®æ”¹çš„å»ºè®®

## é”™åˆ«å­—ä¿®æ”¹  
- ç¬¬Xæ®µï¼š"é”™å­—" â†’ åº”ä¸ºï¼š"æ­£å­—" â†’ ä½ç½®ï¼šå…·ä½“ä½ç½®æè¿°

## æ ‡ç‚¹ç¬¦å·ä¿®æ”¹
- ç¬¬Xæ®µï¼šé—®é¢˜æè¿° â†’ å»ºè®®ï¼šæ­£ç¡®ç”¨æ³•

## è¯­è¨€è¡¨è¾¾æ”¹è¿›å»ºè®®
- å»ºè®®å†…å®¹ï¼ˆç»™å‡ºå»ºè®®å’Œä¾‹å­ï¼Œä½†ä¸ç›´æ¥æä¾›ä¿®æ”¹åçš„åŸæ–‡ï¼‰

## å†…å®¹ç»“æ„æ”¹è¿›å»ºè®®
- å»ºè®®å†…å®¹

ä½œæ–‡å†…å®¹ï¼š
{text}

è¯·å¼€å§‹æ‰¹æ”¹ï¼š"""

        # è°ƒç”¨AIæ¨¡å‹
        response = client.chat.completions.create(
            model=AliyunModel.DEEPSEEK_R1.value,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€åä¸“ä¸šçš„è¯­æ–‡è€å¸ˆï¼Œè´Ÿè´£æ‰¹æ”¹å­¦ç”Ÿä½œæ–‡ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=2000
        )
        
        ai_response = response.choices[0].message.content
        
        # è§£æAIè¿”å›çš„æ‰¹æ”¹ç»“æœ
        corrections = parse_correction_response(ai_response)
        
        return {
            "success": True,
            "corrections": corrections,
            "raw_response": ai_response
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": f"AIæ‰¹æ”¹å¤±è´¥ï¼š{str(e)}"
        }


def ai_correct_essay_stream(text: str):
    """
    ä½¿ç”¨AIæ‰¹æ”¹ä½œæ–‡çš„æµå¼ç‰ˆæœ¬ï¼Œå®æ—¶è¾“å‡ºæ€è€ƒè¿‡ç¨‹
    
    Args:
        text (str): è¦æ‰¹æ”¹çš„ä½œæ–‡å†…å®¹
        
    Yields:
        Dict[str, Any]: åŒ…å«æµå¼è¾“å‡ºçš„å­—å…¸
            - type: 'thinking' | 'result' | 'error'
            - content: æ€è€ƒå†…å®¹ï¼ˆä»…å½“type='thinking'æ—¶ï¼‰
            - corrections: ä¿®æ”¹å»ºè®®åˆ—è¡¨ï¼ˆä»…å½“type='result'æ—¶ï¼‰
            - error: é”™è¯¯ä¿¡æ¯ï¼ˆä»…å½“type='error'æ—¶ï¼‰
    """
    if not text or not text.strip():
        yield {
            "type": "error",
            "error": "ä½œæ–‡å†…å®¹ä¸èƒ½ä¸ºç©º"
        }
        return
    
    try:
        # è·å–é˜¿é‡Œäº‘é…ç½®
        provider = get_provider_config('aliyun')
        client = provider.get_llm()
        
        # æ„å»ºæ‰¹æ”¹æç¤ºè¯
        prompt = f"""è¯·ä½ ä½œä¸ºä¸€åä¸“ä¸šçš„è¯­æ–‡è€å¸ˆï¼Œä»”ç»†æ‰¹æ”¹ä»¥ä¸‹ä½œæ–‡ã€‚è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚ï¼š

1. ä¸è¦ä¿®æ”¹åŸæ–‡ï¼Œåªæ ‡å‡ºéœ€è¦ä¿®æ”¹çš„åœ°æ–¹
2. ç”¨æ¡ç›®çš„æ–¹å¼åˆ—å‡ºä¿®æ”¹æ„è§ï¼Œéœ€è¦åŒ…æ‹¬ï¼š
   - ç—…å¥ï¼ˆè¯­æ³•é”™è¯¯ã€è¡¨è¾¾ä¸å½“ï¼‰
   - é”™åˆ«å­—ï¼ˆé”™å­—ã€åˆ«å­—ï¼‰
   - æ ‡ç‚¹ç¬¦å·é”™è¯¯
   - è¯­è¨€è¡¨è¾¾æ–¹é¢çš„æ”¹è¿›å»ºè®®
   - å†…å®¹ç»“æ„æ–¹é¢çš„æ”¹è¿›å»ºè®®

3. å¯¹äºè¯­è¨€ã€å†…å®¹æ–¹é¢çš„ä¿®æ”¹ï¼Œç»™å‡ºä¿®æ”¹æ„è§å’Œä¾‹å­ï¼Œä½†ä¸è¦ç»™å‡ºå¯ä»¥ç›´æ¥ä½¿ç”¨çš„ä¿®æ”¹ååŸæ–‡ã€‚
4. è¯·ç”¨ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š

## ç—…å¥ä¿®æ”¹
- ç¬¬Xæ®µç¬¬Yå¥ï¼š"åŸæ–‡å†…å®¹" â†’ é—®é¢˜ï¼šå…·ä½“é—®é¢˜æè¿° â†’ å»ºè®®ï¼šå¦‚ä½•ä¿®æ”¹çš„å»ºè®®

## é”™åˆ«å­—ä¿®æ”¹  
- ç¬¬Xæ®µï¼š"é”™å­—" â†’ åº”ä¸ºï¼š"æ­£å­—" â†’ ä½ç½®ï¼šå…·ä½“ä½ç½®æè¿°

## æ ‡ç‚¹ç¬¦å·ä¿®æ”¹
- ç¬¬Xæ®µï¼šé—®é¢˜æè¿° â†’ å»ºè®®ï¼šæ­£ç¡®ç”¨æ³•

## è¯­è¨€è¡¨è¾¾æ”¹è¿›å»ºè®®
- å»ºè®®å†…å®¹ï¼ˆç»™å‡ºå»ºè®®å’Œä¾‹å­ï¼Œä½†ä¸ç›´æ¥æä¾›ä¿®æ”¹åçš„åŸæ–‡ï¼‰

## å†…å®¹ç»“æ„æ”¹è¿›å»ºè®®
- å»ºè®®å†…å®¹

ä½œæ–‡å†…å®¹ï¼š
{text}

è¯·å¼€å§‹æ‰¹æ”¹ï¼š"""

        yield {
            "type": "thinking",
            "content": "æ­£åœ¨è¿æ¥AIæ¨¡å‹..."
        }
        
        # ä½¿ç”¨æµå¼è¾“å‡ºè°ƒç”¨AIæ¨¡å‹
        stream = client.chat.completions.create(
            model=AliyunModel.DEEPSEEK_R1.value,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€åä¸“ä¸šçš„è¯­æ–‡è€å¸ˆï¼Œè´Ÿè´£æ‰¹æ”¹å­¦ç”Ÿä½œæ–‡ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=2000,
            stream=True
        )
        
        yield {
            "type": "thinking",
            "content": "AIå¼€å§‹æ€è€ƒæ‰¹æ”¹æ–¹æ¡ˆ..."
        }
        
        # æ”¶é›†æµå¼å“åº”
        full_response = ""
        current_line = ""
        
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                full_response += content
                current_line += content
                
                # å®æ—¶æ˜¾ç¤ºAIçš„æ€è€ƒå†…å®¹
                if content:
                    yield {
                        "type": "thinking",
                        "content": content
                    }
                
                # å½“é‡åˆ°æ¢è¡Œç¬¦æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦æ˜¯å®Œæ•´çš„æ®µè½æ ‡é¢˜
                if "\n" in current_line:
                    lines = current_line.split("\n")
                    for line in lines[:-1]:  # å¤„ç†é™¤æœ€åä¸€è¡Œå¤–çš„æ‰€æœ‰è¡Œ
                        if line.strip().startswith("##"):
                            section_name = line.strip().replace("##", "").strip()
                            if section_name:
                                yield {
                                    "type": "thinking",
                                    "content": f"\nğŸ“‹ å¼€å§‹åˆ†æï¼š{section_name}\n"
                                }
                    current_line = lines[-1]  # ä¿ç•™æœ€åä¸€è¡Œç»§ç»­å¤„ç†
        
        yield {
            "type": "thinking",
            "content": "AIåˆ†æå®Œæˆï¼Œæ­£åœ¨æ•´ç†æ‰¹æ”¹ç»“æœ..."
        }
        
        # è§£æAIè¿”å›çš„æ‰¹æ”¹ç»“æœ
        corrections = parse_correction_response(full_response)
        
        # å‘é€å®Œæˆä¿¡å·
        yield {
            "type": "thinking",
            "content": "\nâœ… AIæ‰¹æ”¹å®Œæˆï¼\n"
        }
        
        yield {
            "type": "result",
            "corrections": corrections,
            "raw_response": full_response
        }
        
    except Exception as e:
        yield {
            "type": "error",
            "error": f"AIæ‰¹æ”¹å¤±è´¥ï¼š{str(e)}"
        }


def parse_correction_response(response: str) -> List[Dict[str, Any]]:
    """
    è§£æAIæ‰¹æ”¹å“åº”ï¼Œæå–å„ç±»ä¿®æ”¹å»ºè®®
    
    Args:
        response (str): AIçš„æ‰¹æ”¹å“åº”
        
    Returns:
        List[Dict[str, Any]]: è§£æåçš„ä¿®æ”¹å»ºè®®åˆ—è¡¨
    """
    corrections = []
    
    # æŒ‰ç…§ä¸åŒçš„ä¿®æ”¹ç±»å‹åˆ†å‰²å“åº”
    sections = {
        "ç—…å¥ä¿®æ”¹": [],
        "é”™åˆ«å­—ä¿®æ”¹": [],
        "æ ‡ç‚¹ç¬¦å·ä¿®æ”¹": [],
        "è¯­è¨€è¡¨è¾¾æ”¹è¿›å»ºè®®": [],
        "å†…å®¹ç»“æ„æ”¹è¿›å»ºè®®": []
    }
    
    current_section = None
    lines = response.split('\n')
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ–°çš„åˆ†ç±»æ ‡é¢˜
        if line.startswith('##'):
            section_name = line.replace('##', '').strip()
            if section_name in sections:
                current_section = section_name
            continue
        
        # å¦‚æœæ˜¯åˆ—è¡¨é¡¹ï¼Œæ·»åŠ åˆ°å½“å‰åˆ†ç±»
        if line.startswith('-') and current_section:
            correction_text = line[1:].strip()
            if correction_text:
                sections[current_section].append(correction_text)
    
    # å°†å„ä¸ªåˆ†ç±»çš„å†…å®¹æ•´ç†æˆæœ€ç»ˆæ ¼å¼
    for section_name, items in sections.items():
        if items:
            corrections.append({
                "type": section_name,
                "items": items
            })
    
    return corrections


if __name__ == "__main__":
    # æµ‹è¯•å‡½æ•°
    test_text = "ä½ å¥½ï¼Œä¸–ç•Œï¼Hello, world! è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬ã€‚This is a test."
    result = analyze_text(test_text)
    print(format_analysis_result(result)) 